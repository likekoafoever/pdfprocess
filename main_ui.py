import streamlit as st
import os
import re
import pickle
import fitz  # PyMuPDF

from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM
from langchain_ollama import OllamaEmbeddings
import streamlit as st

# ğŸ”§ ì„¤ì •
upload_dir = "upload_docs"
index_path = "faiss_index3"
chunk_cache_path = os.path.join(index_path, "chunks.pkl")
faiss_file = os.path.join(index_path, "index.faiss")
pkl_file = os.path.join(index_path, "index.pkl")


# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
system_prompt = """ë„ˆëŠ” ëŒ€í•œí•œêµ­ ë²•í•™ì „ë¬¸ëŒ€í•™ì›/ë¡œìŠ¤ì¿¨ì˜ ì…ì‹œ ì „ë¬¸ê°€ì•¼. í‘œë¡œ ì •ë¦¬ëœ ëª¨ì§‘ ì¸ì›ë„ ì •í™•íˆ ìˆ«ìë¥¼ ì¶”ì¶œí•´ì„œ ìš”ì•½í•´ì¤˜. ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ë©´, ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•´.

- ë¬¸ì„œê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, ëŒ€í•™ë³„ë¡œ ì •ë³´ë¥¼ ë¶„ë¦¬í•´ì„œ ìš”ì•½í•˜ê³  ë¹„êµí•´ì¤˜.
- 2ê°œ ì´ìƒì˜ ëŒ€í•™ì— ëŒ€í•œ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì¶œì²˜ë¥¼ ì°¸ê³ í•´ì„œ ëŒ€í•™ì„ êµ¬ë¶„í•´ì„œ í•´ì„í•´ì¤˜.
- ë¬¸ì„œ ë‚´ìš©ì´ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëŒ€ë‹µí•´.

ë°˜ë“œì‹œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´."""

embedding_model = OllamaEmbeddings(model="bge-m3")
#embedding_model = OllamaEmbeddings(model="llama3.2")
#embedding_model = OllamaEmbeddings(model="benedict/linkbricks-llama3.1-korean:8b")

#llm = OllamaLLM(model="benedict/linkbricks-llama3.1-korean:8b", system=system_prompt)
#llm = OllamaLLM(model="llama3.2", system=system_prompt)
llm = OllamaLLM(model="gemma3:4b", system=system_prompt)

# ğŸ§  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", """
        ë‹¤ìŒì€ ì—¬ëŸ¬ ëŒ€í•™ì˜ 2025í•™ë…„ë„ ì…ì‹œìš”ê°• ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì •ë³´ì…ë‹ˆë‹¤.
        ì œì‹œëœ ì •ë³´ëŠ” ë²•í•™ì „ë¬¸ëŒ€í•™ì›/ë¡œìŠ¤ì¿¨ì˜ ë‚´ìš©ìœ¼ë¡œ [[start text]] ì •ë³´ [[end text]] ë‚´ìš©ì„ ì°¸ê³ í•˜ì„¸ìš”.
                
        ì¶œì²˜ë¥¼ í™•ì¸í•´ì„œ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ìì„¸íˆ ì‘ë‹µí•´ ì£¼ì„¸ìš”.
        
        {input_documents}
        
        ì§ˆë¬¸: {question}""")
])


def show_all_docs_summary(docs, max_len=1000):
    summaries = []
    summaries.append(f"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}\n")
    for i, doc in enumerate(docs):
        summary = f"--- ë¬¸ì„œ {i+1} ---\n"
        summary += f"ğŸ“Œ ì¶œì²˜: {doc.metadata.get('source')} (í˜ì´ì§€ {doc.metadata.get('page')})\n"
        content = doc.page_content.strip()
        summary += "ğŸ“ ë‚´ìš© ìš”ì•½:\n"
        summary += content[:max_len] + ("..." if len(content) > max_len else "") + "\n"
        summaries.append(summary)
    return "\n".join(summaries)

# ê°€ëŠ¥í•œ ëŒ€í•™ëª… ë¦¬ìŠ¤íŠ¸
UNIVERSITIES = ["ê°•ì›ëŒ€", "ê±´êµ­ëŒ€", "ê²½ë¶ëŒ€", "ê²½í¬ëŒ€", "ê³ ë ¤ëŒ€", "ë™ì•„ëŒ€","ë¶€ì‚°ëŒ€", "ì„œê°•ëŒ€", "ì„œìš¸ëŒ€", "ì„œìš¸ì‹œë¦½ëŒ€", "ì„±ê· ê´€ëŒ€", "ì•„ì£¼ëŒ€", "ì—°ì„¸ëŒ€", "ì˜ë‚¨ëŒ€", "ì›ê´‘ëŒ€", "ì´í™”ì—¬ëŒ€", "ì¸í•˜ëŒ€", "ì „ë‚¨ëŒ€", "ì „ë¶ëŒ€", "ì œì£¼ëŒ€", "ì¤‘ì•™ëŒ€", "ì¶©ë‚¨ëŒ€", "ì¶©ë¶ëŒ€", "í•œêµ­ì™¸ëŒ€", "í•œì–‘ëŒ€"]

def normalize_univ_name(name):
    name = re.sub(r"í•œêµ­ì™¸êµ­ì–´", "í•œêµ­ì™¸ëŒ€", name).strip()
    return re.sub(r"ëŒ€í•™êµ", "ëŒ€", name).strip()

# ëŒ€í•™ë³„ ì§ˆë¬¸ ë¶„ë¦¬ í•¨ìˆ˜
def extract_universities(question: str) -> list:
    return [univ for univ in UNIVERSITIES if normalize_univ_name(univ) in question]

def split_question_by_university(original_question: str, universities: list[str]) -> dict:
    return {
        univ: f"{univ}" + re.sub("|".join(universities), "", original_question)
        for univ in universities
    }

@st.cache_data(show_spinner=False)
def load_chunks():
    if os.path.exists(chunk_cache_path):
        with open(chunk_cache_path, "rb") as f:
            chunks = pickle.load(f)
    #else:
    #    chunks = process_pdfs_to_chunks()
    return chunks


def hash_ignore(obj):
    return None
@st.cache_resource(show_spinner=False, hash_funcs={
    type(lambda x: x): lambda _: None  # í•¨ìˆ˜ ê°ì²´ëŠ” í•´ì‹œí•˜ì§€ ì•ŠìŒ
})
def load_vectorstore(_embedding_model, _chunks):
    if os.path.exists(faiss_file) and os.path.exists(pkl_file):
        vectorstore = FAISS.load_local(index_path, _embedding_model, allow_dangerous_deserialization=True)
    #else:
    #    vectorstore = FAISS.from_documents(_chunks, _embedding_model)
    #    vectorstore.save_local(index_path)
    return vectorstore

def main():
    st.title("ì…ì‹œ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    st.write("ëŒ€í•™ì—ì„œ ê³µê°œí•œ 2025ë…„ ì…ì‹œìš”ê°• ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í•™ë³„ ì…ì‹œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    # st.form ë‚´ë¶€ì—ì„œ ì…ë ¥ ìœ„ì ¯ì„ ì‚¬ìš©í•˜ë©´ ì—”í„°í‚¤ ì…ë ¥ ì‹œì—ë„ í¼ì´ ì œì¶œë©ë‹ˆë‹¤.
    with st.form(key="search_form"):
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆì‹œì™€ ê°™ì´ ì„¸ë¶€ì ì¸ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ ì…ë ¥í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.", "ê°•ì›ëŒ€ ì¼ë°˜ì „í˜•ê³¼ íŠ¹ë³„ì „í˜•ì˜ ëª¨ì§‘ì¸ì›ì„ ì•Œë ¤ì£¼ì„¸ìš”")
        submit = st.form_submit_button("ê²€ìƒ‰ ì‹¤í–‰")

    # ë²¡í„° Store ë¡œë“œ
    chunks = load_chunks()
    vectorstore = load_vectorstore(_embedding_model=embedding_model, _chunks=chunks)
    
    if submit:  #ê²€ìƒ‰ ì‹¤í–‰
        with st.spinner("ëŒ€í•™ë³„ ëª¨ì§‘ìš”ê°• ê²€ìƒ‰ ì¤‘..."):
            univ_names = extract_universities(question)
            split_questions = split_question_by_university(question, univ_names)

            print(question)
            responresponse_one_univ = ""

            if univ_names:
                all_response = []
                refer_docs = []
                for univ in univ_names:
                    univ_question = split_questions[univ]
                    
                    print(univ_question)

                    status = st.empty()
                    status.write(f"ğŸ¯ {univ} ë¬¸ì„œì •ë³´ ê²€ìƒ‰ ì¤‘...")
                    docs = vectorstore.similarity_search(question, k=5, filter={"university": univ})
                    response = create_stuff_documents_chain(
                                llm, prompt, document_variable_name="input_documents"
                            ).invoke({
                                "input_documents": docs,
                                "question": univ_question
                            })
                    response_one_univ = response
                    #st.success(f"âœ… {univ} ê²°ê³¼:")
                    #st.write(response)

                    # ì›í•˜ëŠ” ë©”íƒ€ë°ì´í„° ì„¤ì • (ëŒ€í•™ëª… ê¸°ì¤€ìœ¼ë¡œ)
                    response_doc = Document(
                        page_content=f"ì•„ë˜ëŠ” {univ}í•™êµ ë²•í•™ì „ë¬¸ëŒ€í•™ì›/ë¡œìŠ¤ì¿¨ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. [[start text]] {response} [[end text]]",
                        metadata={"university": univ}  # í˜¹ì€ ë‹¤ë¥¸ ì •ë³´ë„ ì¶”ê°€ ê°€ëŠ¥
                    )
                    all_response.append(response_doc)
                    refer_docs.extend(docs)
                    status.success(f"âœ… {univ} ë¬¸ì„œì •ë³´ ì¶”ì¶œ ì™„ë£Œ!")
                    
                if not all_response:
                    st.error("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëŒ€í•™ëª…ì´ë‚˜ PDF ë°ì´í„°ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
                    return
                
                if len(univ_names) > 1:
                    combine_chain = create_stuff_documents_chain(llm, prompt, document_variable_name="input_documents")
                    response = combine_chain.invoke({
                        "input_documents": all_response,
                        "question": question
                    })
                else:
                    response = response_one_univ
                    
                st.success("âœ… ê²°ê³¼:")
                st.write(response)
                
                st.write("ğŸ“ ì°¸ì¡° ë¬¸ì„œ:")
                seen = set()
                used_docs = []
                for doc in refer_docs:
                    key = (doc.metadata["source"], doc.metadata["page"])
                    if key not in seen:
                        seen.add(key)
                        used_docs.append(f"- {doc.metadata['source']} (í˜ì´ì§€ {doc.metadata['page']})")
                st.write("\n".join(used_docs))
                
                st.write("ë¬¸ì„œ ìš”ì•½:")
                st.text(show_all_docs_summary(refer_docs))
            else:
                st.error("â— ëŒ€í•™ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ êµ¬ì²´í™”í•´ ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
