import os
import re
import pandas as pd
import fitz  # PyMuPDF
import pdfplumber
import pickle   # chunks ìºì‹œ ì €ì¥
from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_ollama import OllamaEmbeddings   # Ollama Embeddings
from langchain_community.vectorstores import FAISS

# ğŸ”§ ì„¤ì •
upload_dir = "upload_docs"
index_path = "faiss_index3"
chunk_cache_path = os.path.join(index_path, "chunks.pkl")
faiss_file = os.path.join(index_path, "index.faiss")
pkl_file = os.path.join(index_path, "index.pkl")

# embedding_model = OllamaEmbeddings(model="bge-m3")
embedding_model = OllamaEmbeddings(model="llama3.2")

# ëŒ€í•™ë³„ ì…ì‹œìš”ê°• chunk ëŒ€ìƒ í˜ì´ì§€ ì •ì˜
chunk_pages = {
    "2025 ê°•ì›ëŒ€.pdf": (3, 18),
    #"2025 ê°•ì›ëŒ€.txt": (1, None),
    "2025 ê±´êµ­ëŒ€.pdf": (3, None),
    "2025 ê²½ë¶ëŒ€.pdf": (2, 8),
    "2025 ê²½í¬ëŒ€.pdf": (11, 20),
    "2025 ê³ ë ¤ëŒ€.pdf": (3, 17),
    "2025 ë™ì•„ëŒ€.pdf": (3, 16),
    "2025 ë¶€ì‚°ëŒ€.pdf": (3, 17),
    "2025 ì„œê°•ëŒ€.pdf": (3, None),
    "2025 ì„œìš¸ëŒ€.pdf": (5, 12),
    "2025 ì„œìš¸ì‹œë¦½ëŒ€.pdf": (4, 17),
    "2025 ì„±ê· ê´€ëŒ€.pdf": (5, 16),
    "2025 ì•„ì£¼ëŒ€.pdf": (2, None),
    "2025 ì—°ì„¸ëŒ€.pdf": (3, 14),
    "2025 ì˜ë‚¨ëŒ€.pdf": (5, 15),
    "2025 ì›ê´‘ëŒ€.pdf": (2, 15),
    "2025 ì´í™”ì—¬ëŒ€.pdf": (2, 10),
    "2025 ì¸í•˜ëŒ€.pdf": (2, 14),
    "2025 ì „ë‚¨ëŒ€.pdf": (3, 15),
    "2025 ì „ë¶ëŒ€.pdf": (3, 12),
    "2025 ì œì£¼ëŒ€.pdf": (3, 16),
    "2025 ì¤‘ì•™ëŒ€.pdf": (2, 14),
    "2025 ì¶©ë‚¨ëŒ€.pdf": (3, 19),
    "2025 ì¶©ë¶ëŒ€.pdf": (3, 22),
    "2025 í•œêµ­ì™¸ëŒ€.pdf": (3, None),
    "2025 í•œì–‘ëŒ€.pdf": (3, 22)
}

def reduce_spaces(text: str) -> str:
    return re.sub(r' {2,}', '', text)

def reduce_spaces_all(text: str) -> str:
    return re.sub(r' +', '', text)

def process_pdfs_to_chunks():
    all_docs = []

    for file_name in os.listdir(upload_dir):
        file_path = os.path.join(upload_dir, file_name)
        ext = os.path.splitext(file_name)[1].lower()
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_name}")

        try:
            if ext == ".pdf":
                all_docs.extend(load_pdf_file(file_path, file_name))
            elif ext == ".txt":
                doc = load_txt_file(file_path, file_name)
                if doc:
                    all_docs.append(doc)
            else:
                print(f"â— ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_name}")
        except Exception as e:
            print(f"âŒ {file_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print("ğŸ§© í˜ì´ì§€ë³„ ì²­í¬ ìƒì„± ì¤‘...")
    splitter = SemanticChunker(embedding_model)
    #splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
    chunks = splitter.split_documents(all_docs)

    print("ğŸ’¾ ìºì‹œ ì €ì¥ ì¤‘...")
    with open(chunk_cache_path, "wb") as f:
        pickle.dump(chunks, f)
    print("âœ… ì™„ë£Œ! ì´ ì²­í¬ ìˆ˜:", len(chunks))
    return chunks

def load_pdf_file(file_path, file_name):
    docs = []
    univ_name = clean_university_name(file_name)

    with fitz.open(file_path) as pdf_fitz, pdfplumber.open(file_path) as pdf_plumber:
        total_pages = len(pdf_fitz)
        start_page, end_page = chunk_pages.get(file_name, (1, None))
        start_idx = max(start_page - 1, 0)
        end_idx = min(end_page if end_page else total_pages, total_pages)

        for page_number in range(start_idx, end_idx):
            text = extract_text_from_pdf(pdf_fitz, page_number)
            tables = extract_tables_from_pdf(pdf_plumber, page_number)
            combined_text = text + "\n" + "\n".join(tables)

            doc = Document(
                page_content=combined_text,
                metadata={"university": univ_name, "source": file_name, "page": page_number + 1}
            )
            docs.append(doc)

    return docs


def load_txt_file(file_path, file_name):
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    if not text:
        return None

    combined_text = reduce_spaces(text)
    return Document(
        page_content=combined_text,
        metadata={"university": clean_university_name(file_name), "source": file_name, "page": 1}
    )


def extract_text_from_pdf(pdf_fitz, page_number):
    try:
        text = pdf_fitz.load_page(page_number).get_text()
        return reduce_spaces(text)
    except Exception as e:
        print(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜ (Page {page_number+1}): {e}")
        return ""


def extract_tables_from_pdf(pdf_plumber, page_number):
    summaries = []
    try:
        tables = pdf_plumber.pages[page_number].extract_tables()
        for table in tables:
            if not table or len(table) < 2:
                continue
            df = pd.DataFrame(table[1:], columns=table[0])
            md_table = df.to_markdown()
            md_table = reduce_spaces_all(md_table)
            summaries.append(f"[TABLE-START]\n{md_table}\n[TABLE-END]")
    except Exception as e:
        print(f"âš ï¸ í‘œ ì¶”ì¶œ ì˜¤ë¥˜ (Page {page_number+1}): {e}")
    return summaries


def clean_university_name(file_name):
    return file_name.replace("2025 ", "").rsplit(".", 1)[0]

# PDFs -> Chunks
chunks = process_pdfs_to_chunks()


print("ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
# ë²¡í„°ìŠ¤í† ì–´ ë§Œë“¤ê¸°
print("ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
vectorstore = FAISS.from_documents(chunks, embedding_model)
vectorstore.save_local(index_path)
print("âœ… ì™„ë£Œ!")
