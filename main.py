import os 
import re
import pdfplumber
import fitz  # PyMuPDF
import pickle   # chunks ìºì‹œ ì €ì¥
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain_ollama import OllamaEmbeddings  # âœ… ìµœì‹  ìœ„ì¹˜
from langchain_ollama import OllamaLLM
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# ğŸ”§ ì„¤ì •
upload_dir = "upload_docs"
index_path = "faiss_index"
chunk_cache_path = os.path.join(index_path, "chunks.pkl")
faiss_file = os.path.join(index_path, "index.faiss")
pkl_file = os.path.join(index_path, "index.pkl")

# âœ… ì˜ˆì‹œ ì§ˆë¬¸
question = "ê³ ë ¤ëŒ€ì™€ ì—°ì„¸ëŒ€ì˜ ëª¨ì§‘ì¸ì›ì„ ë¹„êµí•´ì¤˜"

# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
system_prompt = """ë„ˆëŠ” ëŒ€í•œí•œêµ­ ë²•í•™ì „ë¬¸ëŒ€í•™ì›/ë¡œìŠ¤ì¿¨ì˜ ì…ì‹œ ì „ë¬¸ê°€ì•¼. í‘œë¡œ ì •ë¦¬ëœ ëª¨ì§‘ ì¸ì›ë„ ì •í™•íˆ ìˆ«ìë¥¼ ì¶”ì¶œí•´ì„œ ìš”ì•½í•´ì¤˜. ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ë©´, ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•´.

- ì§ˆë¬¸ì— ëŒ€í•´ í™•ì‹¤í•œ ì •ë³´ê°€ ë¬¸ì„œì— ìˆì„ ê²½ìš°, ìš”ì ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì„œ ì„¤ëª…í•´ì¤˜.
- ì¶œì²˜ ë¬¸ì„œì˜ ë‚´ìš© ì™¸ì—ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆ.
- ì‚¬ìš©ìê°€ íŠ¹ì • ëŒ€í•™(ì˜ˆ: ì—°ì„¸ëŒ€, ê°•ì›ëŒ€)ì„ ì–¸ê¸‰í•˜ë©´ í•´ë‹¹ ëŒ€í•™ì—ë§Œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì‚¬ìš©í•´.
- ëª¨ì§‘ ë‹¨ìœ„, ì „í˜• ìœ í˜•, ì¸ì› ìˆ˜, ì¼ì •, ì§€ì› ìê²© ë“±ì— ê´€í•œ ì§ˆë¬¸ì´ ìì£¼ ë‚˜ì˜¬ ìˆ˜ ìˆì–´.
- ë¬¸ì„œê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, ëŒ€í•™ë³„ë¡œ ì •ë³´ë¥¼ ë¶„ë¦¬í•´ì„œ ìš”ì•½í•˜ê³  ë¹„êµí•´ì¤˜.
- ë¬¸ì„œ ë‚´ìš©ì´ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëŒ€ë‹µí•´.

ë°˜ë“œì‹œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ê°„ëµí•˜ê²Œ ë‹µë³€í•´."""


embedding_model = OllamaEmbeddings(model="bge-m3")
#embedding_model = OllamaEmbeddings(model="gemma3:4b")
#embedding_model = OllamaEmbeddings(model="benedict/linkbricks-llama3.1-korean:8b")

#llm = OllamaLLM(model="benedict/linkbricks-llama3.1-korean:8b", system=system_prompt)
llm = OllamaLLM(model="gemma3:4b", system=system_prompt, temperature=0)


# ğŸ§  LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", """
        ë‹¤ìŒì€ ì—¬ëŸ¬ ëŒ€í•™ì˜ ì…ì‹œ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì •ë³´ì…ë‹ˆë‹¤.
        ê° ë¬¸ì„œëŠ” [ì¶œì²˜: íŒŒì¼ëª…, í˜ì´ì§€ n] ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        
        ì¶œì²˜ë¥¼ í™•ì¸í•´ì„œ ëŒ€í•™ë³„ë¡œ ë‚´ìš©ì„ êµ¬ë¶„í•´ì„œ ì½ê³ , ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
        
        {input_documents}
        
        ì§ˆë¬¸: {question}""")
])

def reduce_spaces(text: str) -> str:
    return re.sub(r' +', '', text)

def join_known_spacing_errors(text: str) -> str:
    replacements = {
        "ëª¨ ì§‘ ì¸ ì›": "ëª¨ì§‘ì¸ì›",
        "ì§€ ì› ì ê²©": "ì§€ì›ìê²©",
        "ì… ì‹œ ìš” ê°•": "ì…ì‹œìš”ê°•",
        "ì¼ ë°˜ ì „ í˜•": "ì¼ë°˜ì „í˜•",
        "íŠ¹ ë³„ ì „ í˜•": "íŠ¹ë³„ì „í˜•",
        "ì„  ë°œ ì¸ ì›": "ì„ ë°œì¸ì›",
        "ì ê²© ìš” ê±´": "ìê²©ìš”ê±´",
        "ëª¨ ì§‘ êµ°": "ëª¨ì§‘êµ°",
        "ë²• í•™ ì „ ë¬¸ ëŒ€ í•™ ì›": "ë²•í•™ì „ë¬¸ëŒ€í•™ì›",
        "ì „ í˜•": "ì „í˜•",
        "ìœ  í˜•": "ìœ í˜•",
    }
    for wrong, correct in replacements.items():
        text = text.replace(wrong, correct)
    return text

#âœ… all_docs ì „ì²´ ë‚´ìš© ìš”ì•½ ì¶œë ¥í•˜ëŠ” ì½”ë“œ
def show_all_docs_summary(docs, max_len=1000):
    print(f"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}\n")
    for i, doc in enumerate(docs):
        print(f"--- ë¬¸ì„œ {i+1} ---")
        print(f"ğŸ“Œ ì¶œì²˜: {doc.metadata.get('source')} (í˜ì´ì§€ {doc.metadata.get('page')})")
        content = doc.page_content.strip()
        print("ğŸ“ ë‚´ìš© ìš”ì•½:")
        print(content[:max_len] + ("..." if len(content) > max_len else ""))
        print()
        
# ëª¨ë“  ê°€ëŠ¥í•œ ëŒ€í•™ëª… ë¦¬ìŠ¤íŠ¸ (í•„ìš”ì‹œ í™•ì¥ ê°€ëŠ¥)
UNIVERSITIES = ["ê°•ì›ëŒ€", "ê±´êµ­ëŒ€", "ê²½ë¶ëŒ€", "ê²½í¬ëŒ€", "ê³ ë ¤ëŒ€", "ë™ì•„ëŒ€","ë¶€ì‚°ëŒ€", "ì„œê°•ëŒ€", "ì„œìš¸ëŒ€", "ì„œìš¸ì‹œë¦½ëŒ€", "ì„±ê· ê´€ëŒ€", "ì•„ì£¼ëŒ€", "ì—°ì„¸ëŒ€", "ì˜ë‚¨ëŒ€", "ì›ê´‘ëŒ€", "ì´í™”ì—¬ëŒ€", "ì¸í•˜ëŒ€", "ì „ë‚¨ëŒ€", "ì „ë¶ëŒ€", "ì œì£¼ëŒ€", "ì¤‘ì•™ëŒ€", "ì¶©ë‚¨ëŒ€", "ì¶©ë¶ëŒ€", "í•œêµ­ì™¸ëŒ€", "í•œì–‘ëŒ€"]

def normalize_univ_name(name):
    name = re.sub(r"í•œêµ­ì™¸êµ­ì–´", "í•œêµ­ì™¸ëŒ€", name).strip()
    return re.sub(r"ëŒ€í•™êµ", "ëŒ€", name).strip()

def extract_universities(question: str) -> list[str]:
    return [univ for univ in UNIVERSITIES if normalize_univ_name(univ) in question]

# âœ… ë¬¸ì„œ â†’ ì²­í¬ ë³€í™˜
def process_pdfs_to_chunks():
    all_docs = []

    for file_name in os.listdir(upload_dir):
        if not file_name.endswith(".pdf"):
            continue

        file_path = os.path.join(upload_dir, file_name)
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_name}")

        # with pdfplumber.open(file_path) as pdf:
        pdf = fitz.open(file_path)
        for page_number in range(len(pdf)):
            page = pdf.load_page(page_number)
            text = page.get_text()
            tables = page.find_tables()
            table_summaries = []

            for table in tables:
                summary = table.extract()
                table_summaries.append(f"[TABLE-START]\n{summary}\n[TABLE-END]")

            combined_text = text + "\n" + "\n".join(table_summaries)
            
            doc = Document(
                page_content=combined_text,
                metadata={
                    "university": file_name.replace("2025 ", "").replace(".pdf", ""),  # ëŒ€í•™ëª…
                    "source": file_name,
                    "page": page_number + 1
                }
            )

            all_docs.append(doc)

    print("PDF í˜ì´ì§€ë³„ ì²­í¬ ìƒì„± ì™„ë£Œ!")
    #splitter = SemanticChunker(embedding_model)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)
    chunks = splitter.split_documents(all_docs)

    print("âœ… ìºì‹œ ì €ì¥")
    with open(chunk_cache_path, "wb") as f:
        pickle.dump(chunks, f)

    return chunks


# ğŸ’¾ FAISS ë²¡í„° ì €ì¥
if os.path.exists(chunk_cache_path):
    print("ğŸ“‚ ì²­í¬ ìºì‹œ ë¡œë“œ ì¤‘...")
    with open(chunk_cache_path, "rb") as f:
        chunks = pickle.load(f)
else:
    print("ğŸ“‚ ì²­í¬ ìƒì„± ì¤‘...")
    chunks = process_pdfs_to_chunks()

# âœ… FAISS ì¸ë±ìŠ¤ ìƒì„±/ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists(faiss_file) and os.path.exists(pkl_file):
    print("ğŸ“‚ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
    vectorstore = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)
else:
    print("ğŸ’¾ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    if chunks:
        vectorstore = FAISS.from_documents(chunks, embedding_model)
        vectorstore.save_local(index_path)
    else:
        raise ValueError("âŒ ìœ íš¨í•œ ë¬¸ì„œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# í•„í„°ë§ ì„¤ì •
univ_names = extract_universities(question)
if univ_names:
    all_docs = []

    for univ in univ_names:
        print(f"\nğŸ¯ {univ} ì •ë³´ ê²€ìƒ‰ ì¤‘...")
        #source_pdf = f"2025 {univ}.pdf"
        # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        docs = vectorstore.similarity_search(question, k=5, filter={"university": univ})
        #retriever = vectorstore.as_retriever(
        #    search_kwargs={"k": 5, "filter": {"university": univ}}
        #)
        ## docs = retriever.get_relevant_documents(question)  # ìµœì‹  ë¬¸ë²• ì‚¬ìš©
        #docs = retriever.invoke(question)
        all_docs.extend(docs)  # âœ… ëª¨ë“  ëŒ€í•™ ê´€ë ¨ ë¬¸ì„œ í•˜ë‚˜ë¡œ ëª¨ìŒ
        
        # Ollama í™œìš© Prompt êµ¬ì„±
    # âœ… combine_documents_chain ìƒì„± (RetrievalQA ëŒ€ì‹ !)
    combine_chain = create_stuff_documents_chain(llm, prompt, document_variable_name="input_documents")
    
    # âœ… ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°©ì§€
    if not all_docs:
        raise ValueError("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëŒ€í•™ëª…ì´ë‚˜ PDF ë°ì´í„°ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    if not question.strip():
        raise ValueError("âŒ ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # âœ… ë¬¸ì„œ ë³‘í•© + ì§ˆë¬¸ ì „ë‹¬
    response = combine_chain.invoke({
        "input_documents": all_docs,
        "question": question
    })
    
    print("\nâœ… ë¹„êµ ê²°ê³¼:")
    print(response)

    # ğŸ“„ ë¬¸ì„œ ì¶œì²˜ ì¶œë ¥ (ì˜µì…˜)
    print("\nğŸ“ ì‚¬ìš©ëœ ë¬¸ì„œ:")
    seen = set()
    for doc in all_docs:
        key = (doc.metadata["source"], doc.metadata["page"])
        if key not in seen:
            seen.add(key)
            print(f"- {doc.metadata['source']} (í˜ì´ì§€ {doc.metadata['page']})")
            
    # show_all_docs_summary(all_docs)
else:
    print("â— ëŒ€í•™ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ êµ¬ì²´í™”í•´ ì£¼ì„¸ìš”.")



